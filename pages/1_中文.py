import streamlit as st
import boto3
from llama_index.core import ( 
    VectorStoreIndex,
    SimpleDirectoryReader,
    Document,
    StorageContext,
    load_index_from_storage
)
from llama_index.core.settings import Settings
from llama_index.llms.bedrock import Bedrock
from llama_index.embeddings.bedrock import BedrockEmbedding, Models

# Clear Chat History fuction
def clear_screen():
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œçƒ¹é¥ªå¤§å¸ˆï¼é—®æˆ‘æœ‰å…³å°è´©æ‘Šä½æŠ•æ ‡çš„é—®é¢˜!"}]

st.set_page_config(page_title="ä¸å°è´©æœºå™¨äººèŠå¤© ğŸ¤–ğŸ’¬", layout="centered", initial_sidebar_state="auto", menu_items=None)
st.sidebar.success("é€‰æ‹©ä¸Šé¢çš„è¯­è¨€")
with st.sidebar.expander("è¿‡å»çš„èŠå¤©è®°å½•", expanded=True):
    st.markdown("""
    `[ 22 Jan 2024 ]`
    ä½ : éå¸¸æ„Ÿè°¢ä½ çš„å¸®åŠ©! ğŸ‘  
    """)
st.sidebar.button('Clear Screen', on_click=clear_screen)
st.image("./images/logo.png")
st.title("ä¸å°è´©æœºå™¨äººèŠå¤© ğŸ¤–ğŸ’¬")
st.markdown('''
<div>
<div style="border: 0.3px solid gray; padding: 10px; border-radius: 10px; margin: 10px 0px;">
<p><b>å°è´©æ‘Šä½æŠ•æ ‡</b><br>
<i>ç¤ºä¾‹é—®é¢˜ï¼šå¿ å¿ ç¾é£Ÿä¸­å¿ƒç†Ÿé£Ÿæ‘Šä½çš„æœ€é«˜å‡ºä»·æ˜¯å¤šå°‘?</i></p>
</div>
<div style="border: 0.3px solid gray; padding: 10px; border-radius: 10px; margin: 10px 0px;">
<p><b>æ ¹æ®é¢„ç®—æ¨èå°è´©æ‘Šä½</b><br>
<i>ç¤ºä¾‹é—®é¢˜ï¼šå¦‚æœæˆ‘çš„é¢„ç®—æ˜¯$500ï¼Œæˆ‘å¯ä»¥æ ‡åˆ°å“ªä¸€ä¸ªå°è´©ä¸­å¿ƒçš„æ‘Šä½?</i></p>
</div>
</div>
''', unsafe_allow_html=True)

# Setup Bedrock
region='us-east-1'

bedrock_runtime = boto3.client(
    service_name='bedrock-runtime',
    region_name=region,
    aws_access_key_id=st.secrets["AWS_ACCESS_ID"],
    aws_secret_access_key=st.secrets["AWS_ACCESS_KEY"]
)

llm = Bedrock(client=bedrock_runtime, model = "anthropic.claude-3-5-sonnet-20240620-v1:0", system_prompt="You are an AI assistant and your job is to answer questions about the data you have. Keep your answers short, concise and do not hallucinate. If the user ask questions that you don't know, apologize and say that you cannot answer.")
embed_model = BedrockEmbedding(client=bedrock_runtime, model = "amazon.titan-embed-text-v1")

Settings.llm = llm
Settings.embed_model = embed_model

@st.cache_resource(show_spinner=False)
def load_data():
  with st.spinner(
    text="å¯åŠ¨äººå·¥æ™ºèƒ½å¼•æ“ã€‚å¯èƒ½è¿˜è¦ç­‰ä¸€ä¸‹..."):
    # load the documents and create the index
    documents = SimpleDirectoryReader(input_dir="data", recursive=True).load_data()
    index = VectorStoreIndex.from_documents(documents)
    return index

# Create Index
index=load_data()

# Initialize the chat messages history        
if "messages" not in st.session_state.keys():
    st.session_state.messages = [
        {"role": "assistant", "content": "ä½ å¥½ï¼Œçƒ¹é¥ªå¤§å¸ˆï¼é—®æˆ‘æœ‰å…³å°è´©æ‘Šä½æŠ•æ ‡çš„é—®é¢˜!"}
    ]

# Initialize the chat engine
if "chat_engine" not in st.session_state.keys(): 
        st.session_state.chat_engine = index.as_chat_engine(chat_mode="condense_question", verbose=True)

# Prompt for user input and save to chat history
if prompt := st.chat_input("é—®æˆ‘é—®é¢˜"): 
    st.session_state.messages.append({"role": "user", "content": prompt})

# Display the prior chat messages
for message in st.session_state.messages: 
    with st.chat_message(message["role"]):
        st.write(message["content"].replace("$", "\$"))

# If last message is not from assistant, generate a new response
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("è¯·ç»™æˆ‘ä¸€åˆ†é’Ÿï¼Œè®©æˆ‘æƒ³æƒ³..."):
            response = st.session_state.chat_engine.chat(prompt)
            st.write(response.response.replace("$", "\$"))
            message = {"role": "assistant", "content": response.response}
            st.session_state.messages.append(message) # Add response to message history
